\documentclass{article}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsmath,amsfonts,amssymb}
\usepackage{hyperref}
\usepackage{geometry}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{amsthm}
\makeatletter
\def\th@definition{%
  \thm@notefont{}% same as heading font
  \normalfont % body font
}
\makeatother
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newtheorem{defn}{Definition}
\newtheorem{note}{Note}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\DeclareMathOperator*{\affspan}{aff}
\DeclareMathOperator*{\vecspan}{span}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\erf}{erf}

\DeclareMathOperator*{\tr}{tr}

\DeclareMathOperator*{\Eop}{\mathbb{E}}
\newcommand{\E}[1]{\ensuremath{\Eop\left[{#1}\right]}}

\DeclareMathOperator*{\probop}{Pr}
\newcommand{\prob}[1]{\ensuremath{\probop\left[{#1}\right]}}

\newcommand{\proj}[1]{\ensuremath{\pi}_{#1}}
\newcommand{\ltwo}[1]{\lVert{#1}\rVert}
\newcommand{\lone}[1]{\lVert{#1}\rVert_1}

\newcommand{\law}{\ensuremath{\xrightarrow{L}}}
\newcommand{\normal}[2]{\ensuremath{\mathcal{N}\left({{#1}},{{#2}}\right)}}
\newcommand{\trans}[1]{\ensuremath{{#1}^{\mathsf{T}}}}
\newcommand{\rminv}[1]{\ensuremath{{#1}^{\mathsf{+}}}}
\newcommand{\comp}[1]{\ensuremath{{#1}^\bot}}
\newcommand{\zero}{\mathbf{0}}
\newcommand{\w}{\mathbf{w}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\z}{\mathbf{z}}
\newcommand{\uu}{\mathbf{u}}

\newcommand{\Tau}{\mathrm{T}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{filecontents}{paper.bib}
@article{hsu2012tail,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham M and Zhang, Tong and others},
  journal={Electron. Commun. Probab},
  volume={17},
  number={52},
  pages={1--6},
  year={2012}
}

@article{dasgupta2003elementary,
  title={An elementary proof of a theorem of Johnson and Lindenstrauss},
  author={Dasgupta, Sanjoy and Gupta, Anupam},
  journal={Random Structures \& Algorithms},
  volume={22},
  number={1},
  pages={60--65},
  year={2003},
  publisher={Wiley Online Library}
}

\end{filecontents}
\immediate\write18{bibtex paper}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Bounding the distance to a Gaussian affine subspace}
\author{Mike Izbicki}

\begin{document}
\maketitle

Let $X \in \mathbb{R}^{d\times m}$ be a random matrix whose entries are distributed according to the standard normal distribution.
Let $A \in \mathbb{R}^{d\times d}$ be an arbitrary matrix,
and $\Sigma=\trans A A$.
Then $A X$ is a matrix whose columns are multivariate normal random vectors with zero mean and covariance $\Sigma$.
We are interested in the distance from the affine hull of the columns of $AX$ to an arbitrary point $\w \in \mathbb{R}^d$.
Specifically, we want to bound this distance as a function of $m$ and the eigenvalues of $\Sigma$.

We use the following notation.
Denote by $\x_i$ the $i$th column of the matrix $X$ and $X_i=(\x_1,...,\x_i)$ the matrix consisting of the first $i$ columns of $X$.
Denote by $V_i$ the span of the columns of $X_i$ and by $H_i$ the affine hull of the columns of $X_i$.
That is,
\begin{equation}
H_i = \affspan\{\x_1,...,\x_i\}=\left\{\sum_{j=1}^i \alpha_j\x_j : \alpha\in\mathbb{R},\sum_{j=1}^i\alpha_j = 1 \right\}
\end{equation}
The projection of a vector $\z$ onto a set $Y$ is the vector
\begin{equation}
\proj{Y}\z = \argmin_{\y\in Y} \ltwo{\y-\z}
\end{equation}
And finally, for any operator $A$ and set $Y$, let the set $AY = \{A\y : \y \in Y\}$.
Using this notation, our goal is to bound
$
\ltwo{\w-\proj{AH_m}\w}
$.
%Finally, if $U$ is a vector space, we denote the orthogonal complement by $\comp U$.

To warm up, we first consider the less general case where $\w$ is the origin.
When $m=1$,
the space $H_1$ is a single point,
so the problem is equivalent to bounding the norm of a Gaussian random vector.
The following lemma gives this bound.
\begin{lemma}[\cite{hsu2012tail}]
\label{lemma:hsu}
Let $\z\in\mathbb{R}^d$ be an isotropic multivariate Gaussian random vector with zero mean.
For all $t>0$,
\begin{equation}
\prob{\ltwo{A\z}^2 > \tr{\Sigma} + 2\sqrt{\tr (\Sigma^2)t} + 2\ltwo{\Sigma}t} \le e^{-t}
\end{equation}
\end{lemma}
\noindent
For $m>1$, the idea is to reduce the problem to finding the norm of a Gaussian random variable in a low dimensional subspace,
then apply Lemma \ref{lemma:hsu}.
This strategy results in the following theorem.
\begin{theorem}
\label{thm:center}
Define the matrix
$
B=
\left(
\comp X_{m-1}
,
\frac{\proj {H_{m-1}} \zero}{\ltwo {\proj {H_{m-1}} \zero}}
\right)
\in \mathbb{R}^{d\times d-m+2}
$
and
$\Tau=\trans B \Sigma B$.
For all $t>0$,
\begin{equation}
\label{eq:thm1a}
\prob{\ltwo{\proj{AH_m}\zero}^2
>
\tr \Tau + 2\sqrt{\tr \Tau t} + 2\ltwo{\Tau}t} \le e^{-t}
\end{equation}
\end{theorem}

\noindent

\begin{proof}
We will first construct a random vector $\z$ such that $\ltwo{\proj{AH_m}\zero} \le \ltwo {A\z} $.
Then we will show that $\z$ has the same distribution as $B\z'$,
where $\z'\in\mathbb{R}^{d-m+2}$ is an isotropic normal distribution.
The result follows by applying Lemma $\ref{lemma:hsu}$ to $B\z'$.

Define
$
\z
=
\x_m +\proj{H_{m-1}}\zero-\proj{H_{m-1}}\proj{V_{m-1}}\x_m
$.
We have that $\z\in H_m$ because $\x_m\in H_m$ and $H_{m-1}\subseteq H_m$.
Therefore $A\z \in AH_m$.
Then by the definition of $\proj{}$,
we have that $\ltwo {\proj{AH_m}\zero}\le\ltwo{A\z}$.

Split $\z$ into two components $\z_1=\proj{\comp U}\x_m$ and $\z_2=\proj{U}\x_m+\proj{G}\zero-\proj{G}\proj{U}\x_m$.
(Notice that $\z=\z_1+\z_2$.)
Let $\z_1'$ be an isotropic normal random vector with $d-m+1$ dimensions;
then the random vectors $\z_1$ and $\comp X_{m-1}\z_1'$ have the same distribution.
Let $z_2$ be a standard normal random variable;
then the vectors $\z_2$ and $\frac{\proj {H_{m-1}} \zero}{\ltwo{\proj {H_{m-1}} \zero}}z_2$ have the same distribution.
The random vectors $\z_1$ and $\z_2$ are not independent,
but they are orthogonal and their lengths are independent.
Therefore
$\z' =
\begin{pmatrix}
\z_1\\
z_2
\end{pmatrix}
$
is an isotropic random vector of dimension $d-m+2$.
\end{proof}

\newpage

\begin{lemma}[\cite{dasgupta2003elementary}]
\label{lemma:dasgupta}
Let $X_1,...,X_d$ be $d$ independent Gaussian \normal{0}{1} random variables,
and let $Y=\frac{1}{|X|}(X_1,...,X_d)$.
Let the vector $Z\in\mathbb{R}^k$ be the projection of $Y$ onto its first $k$ coordinates,
and let $L=|Z|^2$.
Clearly, $\E{L}=k/d$.
If $k<d$, then
\begin{enumerate}
\item If $\beta < 1$, then
\begin{equation}
\prob{L \le \frac{\beta k}{d}}
\le
\beta^{k/2}\left(1+\frac{(1-\beta)k}{d-k}\right)^{(d-k)/2}
\le
\exp \left( \frac{k}{2} (1-\beta+\ln \beta) \right)
\end{equation}
\item If $\beta > 1$, then
\begin{equation}
\prob{L \ge \frac{\beta k}{d}}
\le
\beta^{k/2}\left(1+\frac{(1-\beta)k}{d-k}\right)^{(d-k)/2}
\le
\exp \left( \frac{k}{2} (1-\beta+\ln \beta) \right)
\end{equation}
\end{enumerate}
\end{lemma}


\begin{lemma}
Let $\w^*$ be an arbitrary $d$ dimensional vector,
and $\w_1,...,\w_m$ be a sequence of $m>2$ random $d$-dimensional vectors sampled independently from the isotropic normal distribution.
Define $H= \left\{\sum_{i=1}^m \alpha_i\w_i : \sum_{i=1}^m\alpha_i = 1 \right\}$ to be the smallest hyperplane containing $\w_1,...,\w_m$.
Then for all $\beta>1$ and $t>0$,
%\begin{equation}
%\prob{h^2 < d - m + 2 + 2\sqrt{(d - m + 2)t} + 2t}  \ge 1-e^{-t}
%\end{equation}
%Let $\w_i \sim \normal{0}{I}$ be a sequence of $m$ independent $d$ dimensional random vectors,
%and let $\hat\w$ be an arbitrary point.
%Define $H= \left\{\sum_{i=1}^m \alpha_i\w_i : \sum_{i=1}^m\alpha_i = 1 \right\}$ to be the smallest hyperplane containing all of the $\w_i$s.
%Then,
\begin{multline}
\prob{
    |\w^*-\proj{H}\w^*|^2 \le
    |\w^*|\left(1-\left(\frac{\beta m}{d}\right)\right)
    +
    d - m + 2 + 2\sqrt{(d - m + 2)t} + 2t
}
\\
\ge
(1-e^{-t})
\erf\left(\frac{\beta m}{d}\right)
\left(1 - \exp\left(\frac{m}{2}(1-\beta+\ln\beta)\right)\right)^2
\end{multline}
\end{lemma}
\begin{proof}
Fix $\w_1,...,\w_{m-1}$.
Let $G$ be the smallest hyperplane containing $\w_1,...,\w_{m-1}$,
and $U$ be the corresponding vector subspace.
We have that %by the triangle inequality
\begin{align}
|\w^* - \proj{H}\w^*|
& \le |\w^* - \proj{H}\proj{U}\w^*| & \text{by definition of $\proj{H}$}\\
& \le |\w^* - \proj{U}\w^*| + |\proj{U}\w^* - \proj{H}\proj{U}\w^*| &\text{by triangle ineq.}
\end{align}
We will bound each of these terms separately.

We begin with the first term by noting that the vectors $(\w^* - \proj{U}\w^*)$ and $\proj{U}\w^*$ are orthogonal.
This lets us use the Pythagorean theorem to conclude that
\begin{align}
\label{eq:pythagorean}
|\w^*-\proj{U}\w^*|
= \sqrt{|\w^*|^2 - |\proj{U}\w^*|^2}
\end{align}
%We will now apply Lemma \ref{lemma:dasgupta} to $\proj{U}\w^*$.
The vector $\proj{U}\w^*$ is a fixed vector projected onto a random subspace,
which has the same distribution as a random vector projected onto a fixed subspace.
Therefore, we can apply Lemma \ref{lemma:dasgupta} to get
\begin{equation}
\label{eq:puwstar}
\prob{
    |\proj{U}\w^*|
    \le
    |\w^*|\left(\frac{\beta m}{d}\right)
}
\ge
1 - \exp\left(\frac{m}{2}(1-\beta+\ln\beta)\right)
\end{equation}
Combining Equations \ref{eq:pythagorean} and \ref{eq:puwstar} gives
\begin{equation}
\prob{
    |\w^*-\proj{U}\w^*|
    \le
    |\w^*|\left(1-\left(\frac{\beta m}{d}\right)\right)
}
\ge
1 - \exp\left(\frac{m}{2}(1-\beta+\ln\beta)\right)
\end{equation}

Now for the second term.
Define the line $U^* = \{\alpha\proj{U}\w^* + (1-\alpha)\proj{G}\proj{U}\w^*\}$,
and the point $\x = \proj{U}\w_m - \proj{G}\proj{U}\w_m+ \proj{G}\w^*$.
By construction, we have that $\x\in U^*$ and $\x+\proj{\comp U}\w_m\in H$.
\begin{align}
|\proj{U}\w^* - \proj{H}\proj{U}\w^*|
&\le
|\proj{U}\w^* - \proj{H}\x|
&\text{by definition of $\proj{H}$}
\\
|\proj{U}\w^* - \proj{H}\proj{U}\w^*|^2
&=
|\proj{U}\w^*-\x|^2 + |\x-\proj{H}\x|^2
&\text{by Pythagorean theorem}
%&\le
%|\proj{U}\w^*-\x| + |\x-\proj{H}\x|
%&\text{by triangle ineq.}
\\
&\le
%|\proj{U}\w^*-\x| + |\x-(\x+\proj{\comp U}\w_m)|
|\proj{U}\w^*-\x|^2 + |\x-(\x+\proj{\comp U}\w_m)|^2
&\text{by definition of $\proj{H}$}
\\
&=
%|\proj{U}\w^*-\x| + |\proj{\comp U}\w_m|
|\proj{U}\w^*-\x|^2 + |\proj{\comp U}\w_m|^2
\label{eq:bound}
\end{align}
%Therefore,
%then
%Therefore, the Pythagorean theorem gives
%\begin{equation}
%|\proj{U}\w^* - \proj{H}\proj{U}\w^*|
%=
%\sqrt{|x-\proj{U}\w^*|^2 + |\proj{\comp U}\w_m|^2}
%\end{equation}
%If both vectors in Equation \ref{eq:bound} above were normally distributed,
%we could finish by applying Lemma \ref{lemma:hsu}.
The right vector above is normally distributed,
but the left vector is not.
Our strategy will be to bound the left vector in probability by a normally distributed vector,
then apply Lemma \ref{lemma:hsu} to the result.
%The right vector is normally distributed and can be bounded using Lemma \ref{lemma:hsu}.
%For the left vector, our strategy is to replace it with a vector that is normally distributed with high probability.
In particular,
$|\proj{U^*}\zero -\x|$ has a standard normal distribution,
and $|\proj{U^*}\zero -\x| \ge |\x - \proj{U}\w^*|$
whenever $|\proj{U^*}\zero -\x| \ge |\proj{U*}\zero - \proj{U}\w^*|$.
By the definition of a normal distribution, we have
\begin{align}
\label{eq:erf}
\prob{|\proj{U^*}\zero - \x| \ge \frac{\beta m}{d}}
\ge
\erf\left(\frac{\beta m}{d}\right)
\end{align}
Furthermore, we have that $|\proj{U^*}\zero - \proj{U}\w^*| \le |\proj{U}\w^*|$,
which is upper bounded in probability by Equation \ref{eq:puwstar}.
Combining Equations \ref{eq:puwstar}, \ref{eq:bound}, and \ref{eq:erf} gives:
\begin{multline}
\prob{
    |\proj{U}\w^* - \proj{H}\proj{U}\w^*|^2
    \le
    |\proj{U}\w^*-\x|^2 + |\proj{\comp U}\w_m|^2
}
\\
\ge
\erf\left(\frac{\beta m}{d}\right)
\left(1 - \exp\left(\frac{m}{2}(1-\beta+\ln\beta)\right)\right)
\label{eq:almostdone}
\end{multline}
Now combining Equation \ref{eq:almostdone} above with Lemma \ref{lemma:hsu} gives our final bound on the right hand term:
\begin{multline}
\prob{
    |\proj{U}\w^* - \proj{H}\proj{U}\w^*|^2
    \le
    d - m + 2 + 2\sqrt{(d - m + 2)t} + 2t
}
\\
\ge
(1-e^{-t})
\erf\left(\frac{\beta m}{d}\right)
\left(1 - \exp\left(\frac{m}{2}(1-\beta+\ln\beta)\right)\right)
\end{multline}
%\begin{align*}
%\prob{|\proj{U^*}\zero - \proj{U}\w^*| \ge k|} \ge \\
%\end{align*}

%and $U^* = \vecspan\{\proj{G}\zero\}$.
%Define the point $\x = \proj{U}\w_m - \proj{G}\proj{U}\w_m+ \proj{G}\zero$.
%The vector $\proj{U}\w_m - \proj{G}\proj{U}\w_m$ is in $U^*$,
%so $\x$ is also in $U^*$.
%Since $U^*$ is a line whose direction is independent of $\x$,
%$\x$ is distributed according to a one dimensional standard normal distribution.
%Also by construction, we have that $\x+\proj{\comp U}\w_m \in H$.
%This implies that $h < |\x + \proj{\comp U}\w_m|$.
%This vector has dimension $d - m + 2$ and an isotropic normal distribution.
%Applying Lemma \ref{lemma:hsu} gives the result.

%Let $U$ be the space spanned by $\w_1,...,\w_{m-1}$,
%and $\uu_1,...,\uu_{m-1}$ be an orthogonal basis for $U$.
%For each $\uu_i$, define the line $G_i = \{ \alpha \w_m + (1-\alpha)\uu_i \}$.
%%Let $V$ be the complement space,
%%We can uniquely decompose $\w_m$ into $\w_U + \w_V$, where $\w_U \in U$ and $\w_V \in V$.
%We have that for each $i$,
%\begin{align}
%|\hat\w - \proj{H}\hat\w|
%&\le |\hat\w - \proj{G_i}\hat\w| &\text{because $G_i \subseteq H$} \\
%&\le |\hat\w - \proj{G_i}\proj{U}\hat\w| &\text{by the definition of $\proj{G_i}$}\\
%&\le |\hat\w - \proj{U}\hat\w| + |\proj{U}\hat\w-\proj{G_i}\proj{U}\hat\w| &\text{by triangle inequality}
%\end{align}
%We now bound the two terms separately.
%
%We begin with the first term by noting that the vectors $(\hat\w - \proj{U}\hat\w)$ and $\proj{U}\hat\w$ are orthogonal.
%This lets us use the Pythagorean theorem to conclude that
%\begin{align}
%\label{eq:pythagorean}
%|\hat\w-\proj{U}\hat\w|
%= \sqrt{|\hat\w|^2 - |\proj{U}\hat\w|^2}
%\end{align}
%%We will now apply Lemma \ref{lemma:dasgupta} to $\proj{U}\hat\w$.
%The vector $\proj{U}\hat\w$ is a fixed vector projected onto a random subspace,
%which has the same distribution as a random vector projected onto a fixed subspace.
%Therefore, we can apply Lemma \ref{lemma:dasgupta} to bound $|\proj{U}\hat\w|$.
%Substituting into Equation \ref{eq:pythagorean} gives
%\begin{align}
%|\hat\w-\proj{U}\hat\w|
%%= \sqrt{|\hat\w|^2 - |\proj{U}\hat\w|^2}
%\le \sqrt{|\hat\w|^2 - \left(\frac{\beta m}{d}|\hat\w|\right)^2}
%= |\hat\w|\left(1-\left(\frac{\beta m}{d}\right)\right)
%\end{align}
%for $\beta\ge1$ with probability at least $1-\exp\left(\frac{k}{2}(1-\beta+\ln\beta)\right)$.
%Applying the Pythagorean theorem and Lemma \ref{lemma:dasgupta} to the first term gives
%\begin{align}
%|\hat\w-\proj{U}\hat\w|
%= \sqrt{|\hat\w|^2 - |\proj{U}\hat\w|^2}
%\le \sqrt{|\hat\w|^2 - \left(\frac{\beta m}{d}|\hat\w|\right)^2}
%= |\hat\w|\left(1-\left(\frac{\beta m}{d}\right)\right)
%\end{align}
%with probability at least $1-\exp(\frac{k}{2}(1-\beta+\ln\beta))$.

%Now for the second term.
%Denote by $V$ the orthogonal complement of $U$.
%The vector $\w_m$ can be uniquely decomposed into $\w_U + \w_V$,
%where $\w_U\in U$ and $\w_V \in V$.
%%For the second term, observe that if $|\w_{\uu_i} - \uu_i| > |\proj{U}\hat\w-\uu_i|$
%\begin{align}
%|\proj{U}\hat\w-\proj{G_i}\proj{U}\hat\w|
%&\le
%|\proj{U_i}\hat\w-\proj{G_i}\proj{U}\hat\w|
%&\text{by definition of $\proj{U}$}
%\\
%&\le
%|\w_{U_i} - \proj{G_i}\w_{U_i}|
%&\text{by similarity of triangles}
%\end{align}
\end{proof}

\bibliographystyle{plain}
\bibliography{paper}

\end{document}


